[
  {
    "objectID": "deadmaus.html",
    "href": "deadmaus.html",
    "title": "",
    "section": "",
    "text": "Identifying Missing Links in Wikipedia\nData can be downloaded here\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport numpy as np\n\n\n# Read in the data\narticles = pd.read_csv(\"./data/articles.tsv\", comment='#', sep='    ')\ncategories = pd.read_csv(\"./data/categories.tsv\", comment='#', sep='    ')\nlinks = pd.read_csv(\"./data/links.tsv\", comment='#', sep='  ')\npaths_finished = pd.read_csv(\"./data/paths_finished.tsv\", comment='#', sep='    ')\npaths_unfinished = pd.read_csv(\"./data/paths_unfinished.tsv\", comment='#', sep='    ')\nshortest_path_distance_matrix = pd.read_csv(\"./data/shortest-path-distance-matrix.txt\", comment='#')\npaths_unfinished.head(10)\n\nFileNotFoundError: [Errno 2] No such file or directory: './data/articles.tsv'"
  },
  {
    "objectID": "p2.html",
    "href": "p2.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom urllib.parse import quote, unquote\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\nimport seaborn as sns\nfrom statistics import mean\nfrom matplotlib.colors import LogNorm"
  },
  {
    "objectID": "p2.html#distributions-of-data",
    "href": "p2.html#distributions-of-data",
    "title": "",
    "section": "Distributions of Data",
    "text": "Distributions of Data\n\nIncoming and Outgoing edges in Wikipedia\n\ndef edge_distributions(edges, incoming=True, old=True):\n    '''\n    Plots the distributions of the number of incoming/outgoing edges for each article for the 2009 and 2022 datasets.\n    \n    :param edges: dataframe containing number of edges (either incoming or outgoing)\n    :param incoming: boolean, True if incoming edges, False if outgoing edges\n    :param old: boolean, True if old dataset, False if new dataset\n    '''\n    articles_edges = (articles.copy(deep=True).join(edges).fillna(0))\n\n    if incoming:\n        articles_edges_lt = articles_edges.article\n    else:\n        articles_edges_lt = articles_edges.linkTarget\n    mean_links = articles_edges_lt.mean()\n    max_links = articles_edges_lt.max()\n    num_leaves = (articles_edges_lt == 0).sum()\n    if old:\n        print(f'The mean number of inlinks is: {mean_links:.2f}')\n        print(f'The maximum number of inlinks on any page is: {max_links}. This is the article on {articles_edges_lt[articles_edges_lt==max_links].index[0]}.')\n        print(f'The number of pages without any inlinks is: {num_leaves}')\n    else:\n        print(f'The mean number of inlinks in 2022 is: {mean_links:.2f}')\n        print(f'The maximum number of inlinks in 2022 on any page is: {max_links}. This is the article on {articles_edges_lt[articles_edges_lt==max_links].index[0]}.')\n        print(f'The number of pages without any inlinks in 2022 is: {num_leaves}')\n\n\n    bins = [i for i in range(int(max(articles_edges_lt)+1))]\n    return articles_edges, bins\n\n\n\nInlinks\nDistibutions of inlinks, original vs 2022\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\nplt.suptitle('Inlinks')\nart1, bins1 = edge_distributions(links.groupby(['linkTarget'])['article'].count(), True)\nart2, bins2 = edge_distributions(links22.groupby(['linkTarget'])['article'].count(), True, False)\nart1.hist(bins=bins1, ax=axes[0], log=True, color='blue' ,label='2009')\nart2.hist(bins=bins2, ax=axes[1], log=True, color='orange', label='2022')\naxes[0].set_title('2009')\naxes[1].set_title('2022')\nplt.show()\n\nThe mean number of inlinks is: 26.04\nThe maximum number of inlinks on any page is: 1551.0. This is the article on United_States.\nThe number of pages without any inlinks is: 469\nThe mean number of inlinks in 2022 is: 49.43\nThe maximum number of inlinks in 2022 on any page is: 1064.0. This is the article on United_States.\nThe number of pages without any inlinks in 2022 is: 467\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\nplt.suptitle('Outlinks')\nart1, bins1 = edge_distributions(links.groupby(['article'])['linkTarget'].count(), False)\nart2, bins2 = edge_distributions(links22.groupby(['article'])['linkTarget'].count(), False, False)\nart1.hist(bins=bins1, ax=axes[0], log=True, color='blue' ,label='2009')\nart2.hist(bins=bins2, ax=axes[1], log=True, color='orange', label='2022')\naxes[0].set_title('2009')\naxes[1].set_title('2022')\nplt.show()\n\nThe mean number of inlinks is: 26.04\nThe maximum number of inlinks on any page is: 294.0. This is the article on United_States.\nThe number of pages without any inlinks is: 17\nThe mean number of inlinks in 2022 is: 49.43\nThe maximum number of inlinks in 2022 on any page is: 367.0. This is the article on Europe.\nThe number of pages without any inlinks in 2022 is: 11\n\n\n\n\n\nWe have used log axes to plot the distributions since the distributions are heavily skewed to the right. The distributions are also similar, but we can see that in general pages got more inlinks as well as outlinks in 2022.\n\n\nCategories\nFor our analysis, we have decided to use 15 categories. These categories are general enough to allow us to interpret them but at the same time specific enough to allow us to do a meaningful analysis.\n\n# The main categories that we are interested in (we have chosen these ourselves)\nwiki_2009_categories = pd.read_csv('./wikispeedia_paths-and-graph/categories.tsv', delim_whitespace=True, names=['article', 'category'], comment='#')\n\n# When articles have more than one main category, we choose to sample one.\n# This is perhaps not an optimal method, but it is currently the best we can think of.\nwiki_2009_categories = wiki_2009_categories.groupby('article').sample(1)\nwiki_2009_categories['category'] = wiki_2009_categories['category'].str.extract(r'subject\\.([a-zA-Z]*)')\nwiki_2009_categories = wiki_2009_categories.set_index('article')\narticle_cat = wiki_2009_categories.to_dict()['category']\n\n\n\nGetting Intimate with the Data\nWe want to see how many articles are in each category\n\nlabels, values = zip(*Counter(article_cat.values()).items())\nindexes = np.arange(len(labels))\nwidth = 0.8\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes, labels, rotation=90)\nplt.title(\"Bar Chart of Major Categories\")\nplt.show()\n\n\n\n\nConverting the articles to categories in the links dataframe\n\ncats_df = links.copy(deep=True)\ncats_df['article'] = cats_df['article'].apply(lambda x: article_cat[x] if x in article_cat else 'NA') \ncats_df['linkTarget'] = cats_df['linkTarget'].apply(lambda x: article_cat[x] if x in article_cat else 'NA') \ncats_df.head()\n\n\n\n\n\n  \n    \n      \n      article\n      linkTarget\n    \n  \n  \n    \n      0\n      NA\n      History\n    \n    \n      1\n      NA\n      People\n    \n    \n      2\n      NA\n      NA\n    \n    \n      3\n      NA\n      Citizenship\n    \n    \n      4\n      NA\n      Countries\n    \n  \n\n\n\n\nComputing the adjacency matrix when the articles are converted to categories\n\ncats_df.groupby(['article', 'linkTarget']).size().unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      linkTarget\n      Art\n      Business\n      Citizenship\n      Countries\n      Design\n      Everyday\n      Geography\n      History\n      IT\n      Language\n      Mathematics\n      Music\n      NA\n      People\n      Religion\n      Science\n    \n    \n      article\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Art\n      99\n      5\n      21\n      50\n      29\n      26\n      162\n      111\n      3\n      45\n      2\n      10\n      25\n      140\n      52\n      55\n    \n    \n      Business\n      4\n      314\n      157\n      395\n      59\n      76\n      714\n      119\n      29\n      32\n      12\n      1\n      44\n      78\n      17\n      159\n    \n    \n      Citizenship\n      22\n      152\n      1030\n      931\n      74\n      184\n      1774\n      639\n      34\n      187\n      25\n      11\n      161\n      585\n      227\n      272\n    \n    \n      Countries\n      25\n      268\n      558\n      1282\n      51\n      249\n      2453\n      511\n      11\n      256\n      8\n      37\n      176\n      159\n      217\n      205\n    \n    \n      Design\n      36\n      64\n      108\n      412\n      422\n      91\n      1236\n      376\n      48\n      58\n      11\n      9\n      182\n      211\n      39\n      423\n    \n    \n      Everyday\n      41\n      74\n      207\n      889\n      104\n      1082\n      2073\n      405\n      113\n      310\n      18\n      40\n      223\n      291\n      128\n      1045\n    \n    \n      Geography\n      113\n      667\n      1531\n      3885\n      459\n      1143\n      12148\n      2231\n      41\n      948\n      32\n      123\n      885\n      973\n      672\n      2113\n    \n    \n      History\n      87\n      142\n      788\n      1461\n      289\n      285\n      3608\n      2893\n      27\n      329\n      37\n      34\n      310\n      1421\n      454\n      675\n    \n    \n      IT\n      0\n      35\n      26\n      35\n      36\n      28\n      88\n      9\n      367\n      52\n      27\n      5\n      42\n      53\n      1\n      56\n    \n    \n      Language\n      19\n      28\n      147\n      381\n      45\n      124\n      876\n      343\n      34\n      727\n      11\n      35\n      96\n      403\n      156\n      126\n    \n    \n      Mathematics\n      3\n      12\n      13\n      15\n      21\n      16\n      31\n      53\n      24\n      26\n      197\n      2\n      26\n      96\n      25\n      62\n    \n    \n      Music\n      9\n      11\n      36\n      200\n      25\n      46\n      399\n      88\n      14\n      59\n      1\n      441\n      70\n      154\n      19\n      51\n    \n    \n      NA\n      43\n      144\n      389\n      757\n      211\n      367\n      2335\n      584\n      86\n      261\n      32\n      66\n      371\n      557\n      193\n      958\n    \n    \n      People\n      97\n      154\n      1027\n      1181\n      247\n      474\n      3867\n      1415\n      63\n      614\n      114\n      178\n      613\n      2269\n      745\n      1037\n    \n    \n      Religion\n      19\n      16\n      142\n      207\n      11\n      56\n      525\n      392\n      6\n      163\n      18\n      6\n      69\n      468\n      645\n      166\n    \n    \n      Science\n      54\n      123\n      263\n      1069\n      303\n      919\n      3994\n      586\n      77\n      361\n      94\n      22\n      631\n      733\n      205\n      12042\n    \n  \n\n\n\n\n\nsns.heatmap(data=cats_df.groupby(['article', 'linkTarget']).size().unstack(fill_value=0), norm=LogNorm())\n\n<AxesSubplot:xlabel='linkTarget', ylabel='article'>\n\n\n\n\n\n\n\nSome Observations\n\nThe Geography row is on average lighter than the Design row: This makes sense because the Geography row is more likely to be used by a player since it is easiest to think of when traversing from one page to the other, the player anticipates their next moves to be easier with this.\nDesign to Geography has a darker colour than Geography to Design: This makes sense since people would move from Design, which is a domain with fewer and less natural connections to other categories, to Geography, a domain with many natural connections to other categories, but not the other way around\n\n\n\nCategory-Wise Analysis\nWe check things like- 1. How many articles are there in each category? 2. Number of inlinks and outlinks for each category\n\n# Return list of all articles in a category\ndef cat_wise(cats_link, cat):\n    cats_link = cats_link[cats_link['category'] == cat].index\n    return list(cats_link)\n\n\ndef get_plaintext(name):\n    file_path = './plaintext_articles/' + name + '.txt'\n    with open(file_path, encoding='utf-8') as file:\n        #first 5 lines are comments and titles \n        lines = file.readlines()[5:] \n        return list(map(str.strip, lines))\n\ndef cat_wise_stats(cats_link, cat):\n    titles = cat_wise(wiki_2009_categories, cat)\n    word_count_ls = []\n\n    for title in titles:\n        word_count= sum([len(l.split()) for l in get_plaintext(title)])\n        word_count_ls.append(word_count)\n\n    inc = links.groupby(['linkTarget'])['article'].count()\n    out = links.groupby(['article'])['linkTarget'].count()\n    filt_inc = inc[[x in article_cat and article_cat[x] == cat for x in inc.index]]\n    filt_out = out[[x in article_cat and article_cat[x] == cat for x in out.index]]\n    avg_words = mean(word_count_ls)\n    avg_in = filt_inc.mean()\n    avg_out = filt_out.mean()\n    print(f'The average number of words in a wikipedia article for category \"{cat}\" is: {avg_words:.2f}')\n    print(f'The average number of inlinks for category \"{cat}\" is: {avg_in:.2f}')\n    print(f'The average number of outlinks for category \"{cat}\" is: {avg_out:.2f}')\n    print(f'The average number of words per inlink for category \"{cat}\" is: {avg_words/avg_in:.2f}')\n    print(f'The average number of words per outlink for category \"{cat}\" is: {avg_words/avg_out:.2f}')\n\n\n\nCategory-Wise Analysis\nDesign\n\ncat_wise_stats(links, 'Design')\n\nThe average number of words in a wikipedia article for category \"Design\" is: 3004.68\nThe average number of inlinks for category \"Design\" is: 13.95\nThe average number of outlinks for category \"Design\" is: 17.83\nThe average number of words per inlink for category \"Design\" is: 215.34\nThe average number of words per outlink for category \"Design\" is: 168.54\n\n\nGeography\n\ncat_wise_stats(links, 'Geography')\n\nThe average number of words in a wikipedia article for category \"Geography\" is: 2950.47\nThe average number of inlinks for category \"Geography\" is: 45.81\nThe average number of outlinks for category \"Geography\" is: 33.77\nThe average number of words per inlink for category \"Geography\" is: 64.40\nThe average number of words per outlink for category \"Geography\" is: 87.36\n\n\n\nprint(f\"2009 dataset total links: {len(links)} \\n2022 dataset total links: {len(links22)}\")\n\n2009 dataset total links: 119882 \n2022 dataset total links: 227581\n\n\n\n# Any missing values?\nprint(\"Missing values: \", links.article.isna().sum())\nprint(\"Missing values: \", links22.article.isna().sum())\nprint(\"Missing values: \", links.linkTarget.isna().sum())\nprint(\"Missing values: \", links22.linkTarget.isna().sum())\n\nMissing values:  0\nMissing values:  0\nMissing values:  0\nMissing values:  0\n\n\n\n\nNumber of words in the articles\nWe want to examine the number of words in the articles to get a better understandign of the dataset. Additionally we want to know how the number of links correlate with the number of links and what the relationship between number of links and number of words look like.\n\n# Count the number of words in each article\nls_articles09 = links.article.unique()\nls_articles22 = links22.article.unique()\n\nword_count_ls = []\nfor article in ls_articles09:\n    word_count= sum([len(l.split()) for l in get_plaintext(quote(article))])\n    word_count_ls.append(word_count)\n\n\n# Plot histogram over number of words per article\nimport matplotlib.pyplot as plt\n\nprint(f\"Longest article: {max(word_count_ls)} words \\nShortest article: {min(word_count_ls)} words \\nMedian length: {np.median(word_count_ls)}\")\n\nplt.hist(word_count_ls, bins=100)\nplt.xlabel(\"Number of words in the articles of 2009\")\nplt.ylabel(\"Frequency\")\n\nLongest article: 16348 words \nShortest article: 100 words \nMedian length: 2653.0\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n# Words over number of links. The articles are ordered the same in the word_count_ls as in links dataframe\nword_count_na = np.array(word_count_ls)\nlinks_of_articles = [item for sublist in links.groupby(links.article).count().values for item in sublist]\nwords_over_links = word_count_na/links_of_articles\n\n\nprint(f\"Max word/link ratio: {max(words_over_links)}\")\nplt.hist(words_over_links, bins=100)\nplt.xlabel(\"Words over links per article\")\n\nMax word/link ratio: 7434.0\n\n\nText(0.5, 0, 'Words over links per article')\n\n\n\n\n\n\n\nCorrelations\nWe need to create lists with counts of each article for both columns in the dataset using the below methodology seperatly. This is because there are more unique articles in the article column than in the linkTarget column. Therefore, in a case where an article which exist in the article column does not exist in the linkTarget column, we append a 0.\n\n# incomming vs outgoing 2009\nunique_articles = links.article.unique()\nunique_target = set(links.linkTarget.unique())\noutgoing = []\nincomming = []\n\nfor article in unique_articles:\n    outgoing.append(links.article.value_counts()[article])\n\nfor article in unique_articles:\n    if article in unique_target:\n        incomming.append(links.linkTarget.value_counts()[article])\n    else:\n        incomming.append(0)\n\n\n# incomming vs outgoing 2022\nunique_articles22 = links22.article.unique()\nunique_target22 = set(links22.linkTarget.unique())\noutgoing22 = []\nincomming22 = []\n\nfor article in unique_articles22:\n    outgoing22.append(links22.article.value_counts()[article])\n\nfor article in unique_articles22:\n    if article in unique_target22:\n        incomming22.append(links22.linkTarget.value_counts()[article])\n    else:\n        incomming22.append(0)\n\n\n# Word Count vs Number of links per article\nprint(f\"2009 Correlation 'word count', 'words over links': {round(np.corrcoef(words_over_links, word_count_ls)[0,1],1)}\")\n\n# Incomming vs outgoing 2009\nprint(f\"2009 Correlation 'outgoing links', 'incomming links': {round(np.corrcoef(outgoing, incomming)[0,1], 3)}\")\n\n# Incomming vs outgoing 2022 \nprint(f\"2022 Correlation 'outgoing links', 'incomming links': {round(np.corrcoef(outgoing22, incomming22)[0,1], 3)}\")\n\n2009 Correlation 'word count', 'words over links': 0.4\n2009 Correlation 'outgoing links', 'incomming links': 0.542\n2022 Correlation 'outgoing links', 'incomming links': 0.717\n\n\nInteresting observation: correlation between incomming and outgoing links for each article got stronger between 2009 and 2022."
  },
  {
    "objectID": "p2.html#graph-analysis",
    "href": "p2.html#graph-analysis",
    "title": "",
    "section": "Graph Analysis",
    "text": "Graph Analysis\n\n\nimport pandas as pd\nimport networkx as nx\nfrom node2vec import Node2Vec\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport os\nimport gensim\nimport umap\n\n\nLoading in the data\n\n# Load in the link data\nlinks_2009 = pd.read_csv('./wikispeedia_paths-and-graph/links.tsv', delim_whitespace=True, names=['from', 'to'], comment='#')\nlinks_2022 = pd.read_csv('./wikispeedia_paths-and-graph/links22.tsv', delim_whitespace=True, comment='#')\n\n# Load in the path data of finished games\nwikispeedia_paths = pd.read_csv('./wikispeedia_paths-and-graph/paths_finished.tsv', delim_whitespace=True,\\\n    names=['hashed_ip', 'timestamp', 'duration_sec', 'path', 'rating'], comment='#')\n\nwikispeedia_paths.head()\n\n\n\n\n\n  \n    \n      \n      hashed_ip\n      timestamp\n      duration_sec\n      path\n      rating\n    \n  \n  \n    \n      0\n      6a3701d319fc3754\n      1297740409\n      166\n      14th_century;15th_century;16th_century;Pacific...\n      NaN\n    \n    \n      1\n      3824310e536af032\n      1344753412\n      88\n      14th_century;Europe;Africa;Atlantic_slave_trad...\n      3.0\n    \n    \n      2\n      415612e93584d30e\n      1349298640\n      138\n      14th_century;Niger;Nigeria;British_Empire;Slav...\n      NaN\n    \n    \n      3\n      64dd5cd342e3780c\n      1265613925\n      37\n      14th_century;Renaissance;Ancient_Greece;Greece\n      NaN\n    \n    \n      4\n      015245d773376aab\n      1366730828\n      175\n      14th_century;Italy;Roman_Catholic_Church;HIV;R...\n      3.0\n    \n  \n\n\n\n\n\n\nConstructing the graphs\n\n# Construct the directed graph for 2009 by adding all links as edges\ngraph_2009 = nx.DiGraph()\n\nfor l, r in links_2009.iterrows():\n    graph_2009.add_weighted_edges_from([(r['from'], r['to'], 1)])\n\nprint(f\"The graph from 2009 has {graph_2009.number_of_nodes()} nodes and {graph_2009.number_of_edges()} edges\")\n\nThe graph from 2009 has 4592 nodes and 119882 edges\n\n\n\n# Construct the directed graph for 2022 by adding all links as edges\ngraph_2022 = nx.DiGraph()\n\nfor l, r in links_2022.iterrows():\n    graph_2022.add_weighted_edges_from([(r['linkSource'], r['linkTarget'], 1)])\n\nprint(f\"The graph from 2022 has {graph_2022.number_of_nodes()} nodes and {graph_2022.number_of_edges()} edges\")\n\nThe graph from 2022 has 4593 nodes and 227580 edges\n\n\nWhile the previous two graphs are unweighted, we choose here to construct a weighted graph for the wikispeedia path graph. We opt to do this as it is more inline the idea of certain links having more importance following “human intuition”. Specifically, the weight of each edge is the number of wikispeedia paths that pass through it.\nCurrently we only add edges from finished paths. We will look into potentially adding unfinished paths as well.\n\ngraph_wikispeedia_paths = nx.DiGraph()\n\nn_skipped = 0\n\nfor l, r in wikispeedia_paths.iterrows():\n    path = r['path'].split(';')\n    for i in range(len(path)-1):\n        if path[i] == '<' or path[i+1] == '<':\n            n_skipped += 1\n            continue\n        # If the edge already exists, increment the weight.\n        if graph_wikispeedia_paths.has_edge(path[i], path[i+1]):\n            graph_wikispeedia_paths[path[i]][path[i+1]]['weight'] += 1\n        else:\n            graph_wikispeedia_paths.add_edge(path[i], path[i+1], weight=1)\n\nprint(f\"The graph constructed from the finished paths has {graph_wikispeedia_paths.number_of_nodes()}\"\n      f\" nodes and {graph_wikispeedia_paths.number_of_edges()} edges\")\n\nprint(f\"Skipped {n_skipped} steps with '<'\")\n\nThe graph constructed from the finished paths has 4168 nodes and 50354 edges\nSkipped 36151 steps with '<'\n\n\n\n\nCreating the node embeddings with node2vec\nThe embeddings created here serve as a starting point for further qualitative and quantitative analysis. The hyperparameters chosen are done so according to only a small bit of testing. More specifically, the parameters that we might change are the number of embedding dimension \\(d=128\\), the random walk length, and the number of walks. Walk length has tentatively been set to 10 as it is slightly more than wikispeedia path lengths, and seems to yield good clustering in the subsequent embedding graphs.\n\ndef get_embedding_model(path, graph):\n    \"\"\" \n    Attempts to load model from path if model exists, otherwise trains one\n    annew using `graph` and saves it.\n    \"\"\"\n\n    if os.path.isfile(path):\n        model = gensim.models.word2vec.Word2Vec.load(path)\n    else:\n        n2v = Node2Vec(graph, dimensions=128, walk_length=10, num_walks=2000, workers=8)\n        model = n2v.fit()\n        model.save(path)\n    \n    return model\n\nMODEL_PATH_2009 = './models/n2v_2009.model'\nMODEL_PATH_2022 = './models/n2v_2022.model'\nMODEL_PATH_WIKISPEEDIA_PATHS = './models/n2v_wikispeedia_paths.model'\n\n\n# Get all the embeddings for the analysis\nmodel_2009 = get_embedding_model(MODEL_PATH_2009, graph_2009)\nmodel_2022 = get_embedding_model(MODEL_PATH_2022, graph_2022)\nmodel_wikispeedia_paths = get_embedding_model(MODEL_PATH_WIKISPEEDIA_PATHS, graph_wikispeedia_paths)\n\n\n\nVisualizing the embeddings in low-dimensional space with UMAP\n\nVisualizing according to categories\n\n# Read in the categories for each article and extract the highest level category.\n# I.e. Science if the category is subject.Science.Chemistry\nwiki_2009_categories = pd.read_csv('./wikispeedia_paths-and-graph/categories.tsv', delim_whitespace=True, \\\n    names=['article', 'category'], comment='#')\nwiki_2009_categories.groupby('article').count().hist()\nplt.title('Distribution of number of base categories over all articles')\n\nText(0.5, 1.0, 'Distribution of number of base categories over all articles')\n\n\n\n\n\nHere we can see that a not insignificant minority of articles have two main categories, so we may have to deal with this in a better way in the future.\n\n# Helper function for finding the correct categories for each embedding\n# in the node2vec models.\ndef map_categories(model, article_2_cat):\n    cat = []\n    for k in model.wv.key_to_index.keys():\n        if k in article_2_cat:\n            cat.append(article_2_cat[k])\n        else:\n            cat.append(\"NA\")\n\n    return cat\n\n# Helper function for plotting neat embedding graphs.\ndef plot_embedding(embedding, cat, title):\n    palette = {k:v for (k, v) in zip(set(cat), sns.color_palette('tab20'))}\n    sns.set(rc={'figure.figsize':(8,8)})\n    ax = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], size=1, data=pd.DataFrame({'category':cat}), hue='category', palette=palette)\n    handles, labels = plt.gca().get_legend_handles_labels()\n    tmp = sorted(list(zip(handles, labels)), key=lambda x: x[1])[1:]\n    plt.legend([x[0] for x in tmp],[x[1] for x in tmp])\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(title)\n    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n# Use UMAP to find an appropriate low-dim projection\nreducer = umap.UMAP()\nembedding_2009 = reducer.fit_transform(model_2009.wv.get_normed_vectors())\n\nreducer = umap.UMAP()\nembedding_2022 = reducer.fit_transform(model_2022.wv.get_normed_vectors())\n\nreducer = umap.UMAP()\nembedding_wikispeedia_paths = reducer.fit_transform(model_wikispeedia_paths.wv.get_normed_vectors())\n\n\ncat = map_categories(model_2009, article_cat)\nplot_embedding(embedding_2009, cat, 'Wikipedia 2009 graph embedding')\n\n\n\n\n\ncat = map_categories(model_2022, article_cat)\nplot_embedding(embedding_2022, cat, 'Wikipedia 2022 graph embedding')\n\n\n\n\n\ncat = map_categories(model_wikispeedia_paths, article_cat)\nplot_embedding(embedding_wikispeedia_paths, cat, 'Wikispeedia weighted path graph embedding')\n\n\n\n\nAnalysis: Comparing the three graphs we can see that there are some really interesting category clusterings. Science seems to be a very large part of the corpus for example. We can also see that the Mathematics and IT clusters end up close to each other, nearby the one end of the science cluster. Looking closer at the differences between wikispeedia and the other two we can see that from visual analysis, the wikispeedia graph exibits less tight local clustering with some categories fragmented to a larger extent. Specifically, we observe that some categories like Science, IT continue to be relatively tightly linked, while others that are more general such as Geography and Countries are more scattered.\n\n\nVisualizing according to hub and authority categorization\nIn this part we use the HITS algorithm to figure out if articles are hubs or authorities. An article that is a hub is one that has many outgoing links to authorities, and an article that is an authority has many incoming links from hubs. We choose here to not modify any of the hyperparameters of the HITS algorithm and leave them at their library defaults. Specifically, we here classify an article as a hub if it has a higher hub-score and vice versa.\n\ndef hits_categorize(graph):\n    graph_hits = nx.hits(graph)\n\n    graph_2_hits_cat = {}\n    for (k1, v1), (k2, v2) in zip(graph_hits[0].items(), graph_hits[1].items()):\n        if v1 > v2:\n            graph_2_hits_cat[k1] = 'hub'\n        else:\n            graph_2_hits_cat[k1] = 'authority'\n    \n    return graph_2_hits_cat\n\n\nwiki_2009_2_hits = hits_categorize(graph_2009)\ncat = map_categories(model_2009, wiki_2009_2_hits)\nplot_embedding(embedding_2009, cat, 'Wikipedia 2009 graph embeddings')\n\n/Users/riko/miniconda3/envs/ada/lib/python3.9/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n\n\n\n\n\n\nwiki_2022_2_hits = hits_categorize(graph_2022)\ncat = map_categories(model_2022, wiki_2022_2_hits)\nplot_embedding(embedding_2022, cat, 'Wikipedia 2022 graph embeddings')\n\n/Users/riko/miniconda3/envs/ada/lib/python3.9/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n\n\n\n\n\n\nwikispeedia_2_hits = hits_categorize(graph_wikispeedia_paths)\ncat = map_categories(model_wikispeedia_paths, wikispeedia_2_hits)\nplot_embedding(embedding_wikispeedia_paths, cat, 'Wikispeedia weighted path graph embeddings')\n\n/Users/riko/miniconda3/envs/ada/lib/python3.9/site-packages/networkx/algorithms/link_analysis/hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n\n\n\n\n\nAnalysis: A tentative qualitative analysis shows that the embedding projections generated from the actual 2009 and 2022 graphs exhibit more a clustering behaviour in the authority articles as opposed to in the graph generated from the wikispeedia paths.\n\n\n\nCategory chord diagram\n\nimport holoviews as hv\nfrom holoviews import opts, dim\n\nhv.extension('bokeh')\nhv.output(size=200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\ndef plot_category_chord(graph, article_cat):\n    all_categories = set([v for v in article_cat.values()])\n    category_weights = pd.DataFrame({'source':[], 'target':[], 'value':[]})\n    for c in all_categories:\n        for c2 in all_categories:\n            if c != c2:\n                category_weights = pd.concat([category_weights, pd.DataFrame({'source':[c], 'target':[c2], 'value':[0]})], ignore_index=True)\n\n    for (s, t, w) in graph.edges(data=True):\n        if s in article_cat and t in article_cat:\n            category_weights.loc[(category_weights['source'] == article_cat[s]) & (category_weights['target'] == article_cat[t]), 'value'] += w['weight']\n\n    category_weights = category_weights[category_weights['value'] > 0]\n\n    nodes = hv.Dataset(pd.DataFrame({'category':list(all_categories)}), 'category')\n\n    chord = hv.Chord((category_weights, nodes)).select(value=(5, None))\n    chord.opts(\n        opts.Chord(cmap='Category20', edge_cmap='Category20', edge_color=dim('source').str(), labels='category', node_color=dim('category').str())\n    )\n    return chord\n\n\nplot_category_chord(graph_2009, article_cat)\n\n\n\n\n\n  \n\n\n\n\n\nplot_category_chord(graph_2022, article_cat)\n\n\n\n\n\n  \n\n\n\n\n\nplot_category_chord(graph_wikispeedia_paths, article_cat)"
  }
]